<p align="center">
  <div style="display: flex; justify-content: center; align-items: ;">
    <img width="180" src="./public/Kafka_logo.png" alt="kafka">
    <img width="180" src="./public/airflow_logo.png" alt="airflow">
    <img width="180" src="./public/apache_zookeeper_logo.png" alt="airflow">
  </div>
  <h1 align="center">âš™ï¸Machine Learning and Data Streamingâš™ï¸</h1>
  <p align="center"></p>
</p>

Welcome to the workshop3, Machine learning and Data streaming . This workshop are focuses on processing and managing data obtained by combining a 5 CSV files which have information about happiness score in different countries using the Apache Kafka and Apache Airflow platforms. This project extends the efforts of previous stages where data cleaning (EDA) tasks were performed

## System Requirements ğŸ–¥ï¸

### Docker:
- **Operating System:** Compatible with Windows, macOS, and Linux.
- **Processor:** Should be 64-bit.
- **RAM:** At least 4 GB is recommended.
- **Virtualization:** Enable virtualization in the BIOS (such as "Intel VT-x" or "AMD-V").

### Apache Kafka:
- **64-bit Processor.**
- **RAM:** At least 4 GB is recommended.
- **ZooKeeper:** Up to version 2.8.0, Kafka relied on ZooKeeper for coordination. However, starting from version 2.8.0, Kafka supports a mode without ZooKeeper dependency.
- **Docker:** Docker images for Kafka can be used.

**Ensure your device meets these system requirements before running the project to avoid compatibility issues..**

## Project Structure ğŸ“ƒ

The structure of the directories and files is as follows:

<pre>
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ README.md
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ consumer.py
    â”œâ”€â”€ dbconfig.py
    â”œâ”€â”€ feature.py
    â”œâ”€â”€ producer.py
    â”œâ”€â”€ docs/
    â”‚   â””â”€â”€ Documentation_ws3.pdf
    â”œâ”€â”€ notebooks/
    â”‚   â”œâ”€â”€ EDA_001.ipynb
    â”‚   â”œâ”€â”€ EDA_002.ipynb
    â”‚   â”œâ”€â”€ EDA_003.ipynb
    â”‚   â”œâ”€â”€ figures/
    â”‚   â”‚   â””â”€â”€ ...
    â”‚   â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ 2015.csv
    â”‚   â”œâ”€â”€ 2016.csv
    â”‚   â”œâ”€â”€ 2017.csv
    â”‚   â”œâ”€â”€ 2018.csv
    â”‚   â””â”€â”€ 2019.csv
    â”œâ”€â”€ model/
    â”‚   â””â”€â”€ RandomForestModel.pkl
    â”œâ”€â”€ public/
    â”‚   â”œâ”€â”€ airflow_logo.png
    â”‚   â”œâ”€â”€ kafka_logo.png
    â”‚   â””â”€â”€ apache_zookeeper_logo.png
    â””â”€â”€  config/
        â””â”€â”€ database.json

</pre>



### Folders ğŸ“
- **data ğŸ“Š:** Contains .csv files with the data that will be used during the project.
- **notebooks ğŸ“š:** Contains the Jupyter notebooks with the project's performance.
- **model ğŸ“‚:** Contains the model file that has the project's RandomForestModel done in the Jupyter notebooks.
- **docs ğŸ“™:** Contains the documentation of the whole project, some of them talk about other evidence, so take a look at them if you are interested.

Key files in the root directory are crucial for the project's execution. Without these, the project will not run correctly.

## Installation Requirements âœ”ï¸

The required libraries are listed in the 'requirements.txt' file. Install them using the following command:

pip install -r requirements.txt


## Project Execution ğŸš€

1. Open a terminal and navigate to the desired folder for cloning the repository:
    ```
    cd your_folder
    ```

2. Clone the repository using this command:
    ```
    git clone https://github.com/VanessaSuare/Workshop3
    ```

3. In the 'config' folder, create a file called 'database.ini' with the following content:
    ```
    [postgresql]
    host=localhost
    database=ws3
    user=your_postgres_user
    password=your_postgres_password

Remember create your 'ws3' database in your posgreSQL!

4. Open a command line and execute the following command to start the Docker service:
    ```
    docker compose up
    ```

5. Open a new terminal, (preferably bash) and execute these commands to access into the folder with kafka producer and execute it:
    ```
    python producer.py
    ```

6. With the producer running, execute the kafka consumer in the same folder using this command:
    ```
    python consumer.py
    ```

## Contact ğŸ“§

If you have any questions or need further assistance, feel free to contact me:

- [dayanna.suarez@uao.edu.co](mailto:dayanna.suarez@uao.edu.co)